{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15ff22ab",
   "metadata": {},
   "source": [
    "# RAG with Streamlit for Finance Applications \n",
    "\n",
    "Reference - https://medium.com/predict/crafting-an-ai-powered-chatbot-for-finance-using-rag-langchain-and-streamlit-4384a8076960"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052bc70",
   "metadata": {},
   "source": [
    "## Run below installs one time only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcdb87ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Installs \n",
    "# Install the OpenAI package for access to OpenAI's language models\n",
    "#!pip install openai\n",
    "# Install Langchain, which is essential for natural language processing tasks in our chatbot\n",
    "#!pip install langchain\n",
    "# Install Streamlit, a user-friendly tool for creating web-based chatbot interfaces\n",
    "#!pip install streamlit\n",
    "# Install Streamlit-Chat, an extension for Streamlit that enhances chatbot functionality\n",
    "#!pip install streamlit-chat\n",
    "# Install Faiss-CPU, a library for efficient similarity search and clustering (useful for chatbot operations)\n",
    "#!pip install faiss-cpu\n",
    "# Install PyPDF to enable our chatbot to work with PDF documents\n",
    "#!pip install pypdf\n",
    "# Install Python-Dotenv for managing environment variables securely\n",
    "#!pip install python-dotenv\n",
    "# Install Tiktoken, a useful tool for tokenizing text data\n",
    "#!pip install tiktoken\n",
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faaab1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "#from htmlTemplates import css, bot_template, user_template\n",
    "from langchain.llms import HuggingFaceHub\n",
    "import openai \n",
    "import yaml\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc355e",
   "metadata": {},
   "source": [
    "### Read API Key from YAML file \n",
    "\n",
    "**Notes**\n",
    "- Save your API KEY as a YAML file keep in same directory as this current notebook\n",
    "- Name the file \"keys.yaml\"\n",
    "\n",
    "REFERENCE \n",
    "https://github.com/dylburger/reading-api-key-from-file/blob/master/Keeping%20API%20Keys%20Secret.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25aecb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import openai \n",
    "import yaml\n",
    "\n",
    "OPENAI_CONFIG_FILE = 'auth.yaml'\n",
    "   \n",
    "with open(OPENAI_CONFIG_FILE, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(type(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a7ff92",
   "metadata": {},
   "source": [
    "#### Check Structure of yaml file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e01b68b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"openai\": {\n",
      "        \"access_key\": \"....",\n",
      "        \"paid_access_key\": \"...."\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(config, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d1464ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our API key is: 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\n"
     ]
    }
   ],
   "source": [
    "apikey = config['openai']['access_key']\n",
    "print(\"Our API key is: %s\" % (apikey))\n",
    "\n",
    "OPENAI_API_KEY=apikey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785b2e4d",
   "metadata": {},
   "source": [
    "### Example 1:   Test a Basic prompt completion \n",
    "\n",
    "Let’s now proceed to using the Chat Completions API by providing it with an input prompt, and in this example, we use Hello!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4460cd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Define the user prompt message\n",
    "prompt = \"Hello!\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=apikey,\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f432d5",
   "metadata": {},
   "source": [
    "## Build chatbot core functions with Langchain and FAISS.\n",
    "\n",
    "**KEY STEPS**\n",
    "\n",
    "- Data Loading *(Getting Ready)*\n",
    "- Splitting the Document  *(Dealing with Large Files)*\n",
    "- Generating Vectors  *(Making Text Understandable by Computers)*\n",
    "- Getting context and querying the LLM *(Finding Answers)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821b48a3",
   "metadata": {},
   "source": [
    "### STEP 1 - Data Loading \n",
    "\n",
    "- Langchain provides tools to handle various document types, like PDFs, plain text, and Markdown files.\n",
    "- In this example, we’ll focus on loading these document types\n",
    "\n",
    "### STEP 2 - Doc Splitting \n",
    "\n",
    "- Often, documents can be too large to process at once.\n",
    "- To solve this issue, we’ll split our document into smaller pieces, such as paragraphs or sentences.\n",
    "\n",
    "### Step 3 -  Generating Vectors \n",
    "\n",
    "- To enable our chatbot to work with text, we’ll convert it into numeric representations called vectors\n",
    "- We’ll use OpenAI embeddings to perform this conversion and then store these vectors in  FAISS vector store.\n",
    "\n",
    "### Step 4 Getting context and querying the LLM \n",
    "- We’ll use the RetrievalQA chain from Langchain to search our FAISS vector store for relevant information.\n",
    "- Once we’ve found the right data, we will query the LLM to provide an answer based on what we’ve retrieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ffe6f1",
   "metadata": {},
   "source": [
    "### Function for STEPs 1,2,3 \n",
    "\n",
    "It takes the document you want to work with and stores it in a local FAISS vector store.\n",
    "\n",
    "#### Key steps executed by the function get_faiss_vectordb\n",
    "\n",
    "- take  filename as input \n",
    "- initiate openAI\n",
    "- create FAISS index path \n",
    "- determine appropriate data loader based on file extension \n",
    "- load document using the selected loader \n",
    "- We use a method of context aware chunking - **Recursive Character Text Splitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25115a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faiss_vectordb(file: str):\n",
    "    # Extract the filename and file extension from the input 'file' parameter.\n",
    "    filename, file_extension = os.path.splitext(file)\n",
    "\n",
    "    # Initiate embeddings using OpenAI.\n",
    "    embedding = OpenAIEmbeddings()\n",
    "\n",
    "    # Create a unique FAISS index path based on the input file's name.\n",
    "    faiss_index_path = f\"faiss_index_{filename}\"\n",
    "\n",
    "    # Determine the loader based on the file extension.\n",
    "    if file_extension == \".pdf\":\n",
    "        loader = PyPDFLoader(file_path=file)\n",
    "    elif file_extension == \".txt\":\n",
    "        loader = TextLoader(file_path=file)\n",
    "    elif file_extension == \".md\":\n",
    "        loader = UnstructuredMarkdownLoader(file_path=file)\n",
    "    else:\n",
    "        # If the document type is not supported, print a message and return None.\n",
    "        print(\"This document type is not supported.\")\n",
    "        return None\n",
    "\n",
    "    # Load the document using the selected loader.\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split the loaded text into smaller chunks for processing.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=30,\n",
    "        separators=[\"\\n\", \"\\n\\n\", \"(?<=\\. )\", \"\", \" \"],\n",
    "    )\n",
    "    doc_chunked = text_splitter.split_documents(documents=documents)\n",
    "\n",
    "    # Create a FAISS vector database from the chunked documents and embeddings.\n",
    "    vectordb = FAISS.from_documents(doc_chunked, embedding)\n",
    "    \n",
    "    # Save the FAISS vector database locally using the generated index path.\n",
    "    vectordb.save_local(faiss_index_path)\n",
    "    \n",
    "    # Return the FAISS vector database.\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb883b2",
   "metadata": {},
   "source": [
    "### STEP 4 Function for Calling LLM With Relevant Context\n",
    "\n",
    "It asks the Large Language Model (LLM) to provide answers based on the context of the user’s question.\n",
    "Langchain is used here \n",
    "\n",
    "\n",
    "REFERENCE\n",
    "\n",
    "https://docs.smith.langchain.com/cookbook/hub-examples/retrieval-qa-chain\n",
    "\n",
    "https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html\n",
    "\n",
    "\n",
    "#### Key steps for function run_llm() \n",
    "\n",
    "- create an instance of ChatOpen AI \n",
    "- create a RetrievalQA instance from chain types\n",
    "- run retriever over the created  vectordb \n",
    "- run query on the RetrievalQA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71dd795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm(vectordb, query: str) -> str:\n",
    "    # Create an instance of the ChatOpenAI with specified settings.\n",
    "    openai_llm = ChatOpenAI(temperature=0, verbose=True)\n",
    "    \n",
    "    # Create a RetrievalQA instance from a chain type with a specified retriever.\n",
    "    retrieval_qa = RetrievalQA.from_chain_type(\n",
    "        llm=openai_llm, chain_type=\"stuff\", retriever=vectordb.as_retriever()\n",
    "    )\n",
    "    \n",
    "    # Run a query using the RetrievalQA instance.\n",
    "    answer = retrieval_qa.run(query)\n",
    "    \n",
    "    # Return the answer obtained from the query.\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e8b824",
   "metadata": {},
   "source": [
    "## UI with Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f955d065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 17:30:28.244 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/anishroychowdhury/anaconda3/envs/LLM/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries.\n",
    "import streamlit as st\n",
    "import backend\n",
    "\n",
    "#from backend.core import run_llm, get_faiss_vectordb\n",
    "\n",
    "# Set the title for the Streamlit app.\n",
    "st.title(\"📝 File GPT using Open AI GPT 3.5 model\")\n",
    "\n",
    "# Allow the user to upload a file with supported extensions.\n",
    "uploaded_file = st.file_uploader(\"Upload an article\", type=(\"txt\", \"md\", \"pdf\"))\n",
    "\n",
    "# Provide a text input field for the user to ask questions about the uploaded article.\n",
    "question = st.text_input(\n",
    "    \"Ask something about the article\",\n",
    "    placeholder=\"Can you give me a short summary?\",\n",
    "    disabled=not uploaded_file,\n",
    ")\n",
    "\n",
    "# If an uploaded file is available, process it.\n",
    "if uploaded_file:\n",
    "    # Save the uploaded file locally.\n",
    "    with open(uploaded_file.name, \"wb\") as f:\n",
    "        f.write(uploaded_file.getbuffer())\n",
    "    \n",
    "    # Create a FAISS vector database from the uploaded file.\n",
    "    vectordb = get_faiss_vectordb(uploaded_file.name)\n",
    "    \n",
    "    # If the vector database is not created (unsupported file type), display an error message.\n",
    "    if vectordb is None:\n",
    "        st.error(\n",
    "            f\"The {uploaded_file.type} is not supported. Please load a file in pdf, txt, or md\"\n",
    "        )\n",
    "\n",
    "# Display a spinner while generating a response.\n",
    "with st.spinner(\"Generating response...\"):\n",
    "    # If both an uploaded file and a question are available, run the model to get an answer.\n",
    "    if uploaded_file and question:\n",
    "        answer = run_llm(vectordb=vectordb, query=question)\n",
    "        # Display the answer in a Markdown header format.\n",
    "        st.write(\"### Answer\")\n",
    "        st.write(f\"{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b150a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
